# -*- coding: utf-8 -*-
"""planttraits2024_submit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/planttraits2024-submit-d44b138d-f38c-4832-ad7a-89ec89348f29.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240425/auto/storage/goog4_request%26X-Goog-Date%3D20240425T202036Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D310ec90b36fadf6b9777748e76dd9a98fd9f11fd28165c768197872411c5cf009cf3487b0a469b1a3cd936a7ec053e680d440516c53e42338a41e87b3975fb768092cf9be53c557d8206b000bbc0a7dc96d06ba1868026449d9e4128de76fae1adc1427d4b12f7181ca5137e281062d2f84cc761e102e63dac261c7c35a67087611f6ee2348f8df217ae7f17b0bddb66dcbc240a5d855c9c3d0abd09c6ecd64dc0a7f531a5df78682467b2057b0ffc94e7441d32b6fd3f5bf00a4faba1f98dd7c89aaba3e7d491247b0c506201665f0d821a4ebba89d4d16ee91b227e66a7b50d7219d1af7cf7b2fb3ee4923ad150a365597906d666e923c565080f40d2e5fc7
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

from torch_base2_v2 import *

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

# CHUNK_SIZE = 40960
# DATA_SOURCE_MAPPING = 'planttraits2024:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F65626%2F8046133%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240425%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240425T202036Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D18bebea9e1d746457bb6a0fa2071f197ccb8d406a521ec06c89caf9208371e6bc481f065cb6fea5f87bf7ce96d213c281c8423a350148de1eae2f3e3e9e9c84e688898008f0baafb66cc8d274e4a49bbdd4a26b8905624f67a2cd79bb1bcf4bf558c55790086d798a0ad44f2bafca9bae8549f00d533bae63f14be197a929725932d62b3f0dd6c25b244698474fd4530262c9e28af2a0627d9c3644756be76d7b3c06686f81c9085831d383f95df208ae8b17a0fadb1870ef1e1270eb4b495769dc28a88bf8cd593a64e019275992a3a74087920641c94238097b59ed3007ae9e560b5adb284ff99faec46f61e28bc73e4fa81aae2280907108db01992ffbede'

# KAGGLE_INPUT_PATH='/kaggle/input'
# KAGGLE_WORKING_PATH='/kaggle/working'
# KAGGLE_SYMLINK='kaggle'

# !umount /kaggle/input/ 2> /dev/null
# shutil.rmtree('/kaggle/input', ignore_errors=True)
# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

# try:
#   os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
# except FileExistsError:
#   pass
# try:
#   os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
# except FileExistsError:
#   pass

# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
#     directory, download_url_encoded = data_source_mapping.split(':')
#     download_url = unquote(download_url_encoded)
#     filename = urlparse(download_url).path
#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
#     try:
#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
#             total_length = fileres.headers['content-length']
#             print(f'Downloading {directory}, {total_length} bytes compressed')
#             dl = 0
#             data = fileres.read(CHUNK_SIZE)
#             while len(data) > 0:
#                 dl += len(data)
#                 tfile.write(data)
#                 done = int(50 * dl / int(total_length))
#                 sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
#                 sys.stdout.flush()
#                 data = fileres.read(CHUNK_SIZE)
#             if filename.endswith('.zip'):
#               with ZipFile(tfile) as zfile:
#                 zfile.extractall(destination_path)
#             else:
#               with tarfile.open(tfile.name) as tarfile:
#                 tarfile.extractall(destination_path)
#             print(f'\nDownloaded and uncompressed: {directory}')
#     except HTTPError as e:
#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
#         continue
#     except OSError as e:
#         print(f'Failed to load {download_url} to path {destination_path}')
#         continue

# print('Data source import complete.')

# import torch
# from torch import nn
# from torchvision import models
# from torchvision import transforms

# from tqdm import tqdm

# import pandas as pd
# from torch.utils.data import DataLoader, Dataset
# import os

# from PIL import Image

# import numpy as np

# ipt = '/kaggle/input/planttraits2024'
# opt = '/kaggle/working/'
# netpth = '/kaggle/input/planttrails2024efficient/model.pth'
# os.environ['CUDA_VISIBLE_DEVICES'] = "0, 1"

# class MultiModalNetwork(nn.Module):
#     def __init__(self, num_classes=6):
#         super(MultiModalNetwork, self).__init__()
#         # 画像のためのバックボーンモデル
#         self.efficientnet_backbone = models.efficientnet_b0(pretrained=False)
#         self.efficientnet_backbone = nn.Sequential(*list(self.efficientnet_backbone.children())[:-1])  # 最後の分類層を除外
#         self.pooling = nn.AdaptiveAvgPool2d(1)
#         self.dropout1 = nn.Dropout(0.5)

#         # 数値データのためのレイヤー
#         self.dense1 = nn.Linear(155, 326)
#         self.dense2 = nn.Linear(326, 64)
#         self.dropout2 = nn.Dropout(0.5)

#         # 結合後のレイヤー
#         self.concat_dense = nn.Linear(1280 + 64, num_classes)

#     def forward(self, image_input, numerical_input):
#         # 画像のためのパス
#         x1 = self.efficientnet_backbone(image_input)
#         x1 = self.pooling(x1).view(-1, x1.shape[1])  # Flatten
#         x1 = self.dropout1(x1)

#         # 数値データのためのパス
#         x2 = nn.functional.relu(self.dense1(numerical_input))
#         x2 = nn.functional.relu(self.dense2(x2))
#         x2 = self.dropout2(x2)

#         # 結合
#         x = torch.cat((x1, x2), dim=1)

#         # 最終的な分類
#         x = self.concat_dense(x)

#         return x


# Assuming that the train.csv is already loaded into the environment
# and we have a PyTorch model defined and trained called 'MyModel'.
# Replace 'MyModel' and its parameters with your actual model.

# Define the dataset
# class PlantTraitsDataset(Dataset):
#     def __init__(self, csv_file, train=True, transform=None):
#         self.transform = transform
#         self.train = train
#         # Load the dataset
#         df = pd.read_csv(csv_file)
#         # Ignore columns 'FQ' (156) to 'FT' (173)
#         self.ids = df['id']
#         self.features = df.iloc[:, 1:156]  # Select columns from 'A' to 'FP' (0 to 155)
#         self.features = (self.features - self.features.mean(axis=0)) / self.features.std(axis=0)

#         self.labels = df.iloc[:, 158:164]  # Select columns for the labels 'FI' to 'FN' (158 to 163)
#         self.std = self.labels.std(axis=0)
#         self.mean = self.labels.mean(axis=0)
#         self.labels = (self.labels - self.mean) / self.std
#     def __len__(self):
#         return len(self.features)

#     def __getitem__(self, idx):
#         # Load the image
#         if self.train:
#             image_path = os.path.join(ipt, 'train_images', str(self.ids.iloc[idx]) + '.jpeg')
#         else:
#             image_path = os.path.join(ipt, 'test_images', str(self.ids.iloc[idx]) + '.jpeg')
#         image = Image.open(image_path)
#         if self.transform:
#             image = self.transform(image)
#         # Load the features and labels
#         features = torch.tensor(self.features.iloc[idx].values.astype('float32'))
#         if self.train:
#             labels = torch.tensor(self.labels.iloc[idx].values.astype('float32'))
#             return image, features, labels
#         else:
#             return image, features

# transform
# train_transform = transforms.Compose([
#     transforms.Resize(256),
#     transforms.CenterCrop(224),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.4482, 0.4525, 0.3360], std=[0.1086, 0.0971, 0.1172]),
# ])
# # transform
# test_transform = transforms.Compose([
#     transforms.Resize(256),
#     transforms.CenterCrop(224),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.4482, 0.4525, 0.3360], std=[0.1086, 0.0971, 0.1172]),
# ])
BASE_PATH = '../planttraits2024/'

# Test
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')
test_df['image_path'] = f'{BASE_PATH}/test_images/'+test_df['id'].astype(str)+'.jpeg'
FEATURE_COLS = test_df.columns[1:-1].tolist()
print(test_df.head(2))

skf = StratifiedKFold(n_splits=CFG.num_folds, shuffle=True, random_state=42)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CustomModel(CFG.num_classes, CFG.aux_num_classes, FEATURE_COLS, model_name=CFG.model_name)
model.to(device)

model.load_state_dict(torch.load('../resnet50.a1_in1k_best_model2_2.pth'))

test_paths = test_df.image_path.values
test_features = scaler.transform(test_df[FEATURE_COLS].values) 
test_ds = build_dataset(test_paths, test_features, batch_size=CFG.batch_size,
                         repeat=False, shuffle=False, augment=False, cache=False)

model.eval()

for batch_idx, inputs_dict in enumerate(tqdm(test_ds, desc='Testing')):
    # Extract images and features from the inputs_dict
    inputs_images = inputs_dict['images'].to(device, dtype=torch.float32)  # Assuming 'device' is the target device
    inputs_features = inputs_dict['features'].to(device, dtype=torch.float32)

    # Forward pass
    with torch.no_grad():
        outputs = model(inputs_images, inputs_features)

    # Get predictions
    predictions = outputs['head'].cpu().numpy()  # Assuming 'head' is the main task output

    # Append predictions to the list
    all_predictions.append(predictions)

# Concatenate predictions for all batches
all_predictions = np.concatenate(all_predictions, axis=0)

pred_df = test_df[["id"]].copy()
target_cols = [x.replace("_mean","") for x in CFG.class_names]
pred_df[target_cols] = all_predictions.tolist()

sub_df = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')
sub_df = sub_df[["id"]].copy()
sub_df = sub_df.merge(pred_df, on="id", how="left")
sub_df.to_csv("sub.csv", index=False)
sub_df

# # Make predictions
# # predictions = []
# # with torch.no_grad():
# #     with tqdm(total=len(test_loader)) as pbar:
# #         for i, (image, numerical) in enumerate(test_loader):
# #             image = image.to(device)
# #             numerical = numerical.to(device)
# #             output = model(image, numerical)
# #             pbar.update(1)
# #             predictions.append(output.detach().cpu().numpy())

# means = np.array(mean)
# stds = np.array(std)
# print(f'mean:{means} stds:{stds}')
# print(stds.shape)
# predictions = np.concatenate(predictions, axis=0)
# print(predictions.shape)
# predictions = (predictions * stds ) + means

# # Assuming the 'id' column is the first column in the train.csv
# ids = pd.read_csv(os.path.join(ipt, 'sample_submission.csv'))['id']

# # Create the submission DataFrame
# submission_df = pd.DataFrame(predictions, columns=['X4', 'X11', 'X18', 'X50', 'X26', 'X3112'])
# submission_df.insert(0, 'id', ids)

# # Save the submission file
# submission_path = 'submission.csv'
# submission_df.to_csv(submission_path, index=False)
# submission_df

# test = pd.read_csv('/kaggle/input/planttraits2024/test.csv')
# train =pd.read_csv('/kaggle/input/planttraits2024/train.csv')

# train = train[train['X4_mean'] >= 0]
# y_columns = [col for col in train.columns if col.endswith('_mean')]
# target = train[y_columns]
# # Rearrange columns in the target DataFrame
# target = target[['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']]

# filtered_train = target[target['X4_mean'] >= 0]
# filtered_train = filtered_train[filtered_train['X11_mean'] < 100]
# filtered_train= filtered_train[filtered_train['X18_mean'] < 50]
# filtered_train= filtered_train[filtered_train['X26_mean'] < 5000]
# filtered_train= filtered_train[filtered_train['X50_mean'] < 10]
# filtered_train[filtered_train['X3112_mean'] < 25000]

# mean_values = filtered_train.mean()
# prediction_columns = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']

# submission = pd.DataFrame({'id': test['id']})
# submission[prediction_columns] = mean_values
# submission

# submission_df = submission[prediction_columns] + submission_df[prediction_columns]
# submission_df.insert(0, 'id', ids)

# submission_df

# submission_df.to_csv(submission_path, index=False)